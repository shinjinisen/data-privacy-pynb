{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMzHoPogqK4B06S98kJxyIi",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shinjinisen/data-privacy-pynb/blob/main/sampledatastet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s93G1zNi79-3",
        "outputId": "4608cab4-716f-40bc-a12c-8c37e8ec97ea"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: recordlinkage in /usr/local/lib/python3.10/dist-packages (0.16)\n",
            "Requirement already satisfied: jellyfish>=1 in /usr/local/lib/python3.10/dist-packages (from recordlinkage) (1.0.3)\n",
            "Requirement already satisfied: numpy>=1.13 in /usr/local/lib/python3.10/dist-packages (from recordlinkage) (1.25.2)\n",
            "Requirement already satisfied: pandas<3,>=1 in /usr/local/lib/python3.10/dist-packages (from recordlinkage) (2.0.3)\n",
            "Requirement already satisfied: scipy>=1 in /usr/local/lib/python3.10/dist-packages (from recordlinkage) (1.11.4)\n",
            "Requirement already satisfied: scikit-learn>=1 in /usr/local/lib/python3.10/dist-packages (from recordlinkage) (1.2.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from recordlinkage) (1.4.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas<3,>=1->recordlinkage) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas<3,>=1->recordlinkage) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas<3,>=1->recordlinkage) (2024.1)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=1->recordlinkage) (3.4.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas<3,>=1->recordlinkage) (1.16.0)\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "!pip install recordlinkage"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We create two datasets, dataset_A and dataset_B, each containing information about individuals."
      ],
      "metadata": {
        "id": "PU2U2kWcA4wj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Sample dataset A\n",
        "data_A = {\n",
        "    'ID': [1, 2, 3, 4],\n",
        "    'Name': ['John Smith', 'Alice Johnson', 'Rabert Brown', 'Emma Liu'],\n",
        "    'Age': [30, 25, 35, 28],\n",
        "    'Address': ['123 Main St', '456 Elm St', '689 Oak St', '101 Pine St'],\n",
        "    'Phone': ['111-111-1111', '222-222-2222', '334-333-3333', '944-444-4444']\n",
        "}\n",
        "\n",
        "dataset_A = pd.DataFrame(data_A)\n",
        "\n",
        "# Sample dataset B\n",
        "data_B = {\n",
        "    'ID': [5, 6, 7, 8],\n",
        "    'Name': ['John Smith', 'Alice Johnson', 'Michael Clark', 'Emma Lee'],\n",
        "    'Age': [30, 26, 35, 27],\n",
        "    'Address': ['123 Main St', '456 Elm St', '789 Oak St', '101 Pine St'],\n",
        "    'Phone': ['111-111-1111', '222-222-2222', '333-333-3333', '555-555-5555']\n",
        "}\n",
        "\n",
        "dataset_B = pd.DataFrame(data_B)"
      ],
      "metadata": {
        "id": "avIl1hplAzlm"
      },
      "execution_count": 151,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that we have our sample datasets, let's proceed with the record linkage steps:\n",
        "\n",
        "**1. Data Preparation:**\n",
        "\n",
        "In this step, we prepare the datasets for record linkage by cleaning and transforming them as needed. This may involve:\n",
        "\n",
        "- Removing irrelevant columns.\n",
        "\n",
        "- Standardizing data formats (e.g., converting all text to lowercase).\n",
        "Handling missing values.\n",
        "\n",
        "- Normalizing data (e.g., removing accents, standardizing date formats)."
      ],
      "metadata": {
        "id": "RB8BccVAA-Mo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert text to lowercase\n",
        "dataset_A['Name'] = dataset_A['Name'].str.lower()\n",
        "dataset_A['Address'] = dataset_A['Address'].str.lower()\n",
        "\n",
        "dataset_B['Name'] = dataset_B['Name'].str.lower()\n",
        "dataset_B['Address'] = dataset_B['Address'].str.lower()\n",
        "\n",
        "# Handle missing values if any\n",
        "dataset_A.dropna(inplace=True)\n",
        "dataset_B.dropna(inplace=True)\n",
        "\n",
        "# Normalize data\n",
        "# For example, remove accents from text data\n",
        "import unicodedata\n",
        "dataset_A['Name'] = dataset_A['Name'].apply(lambda x: unicodedata.normalize('NFKD', x).encode('ASCII', 'ignore').decode())\n",
        "dataset_B['Name'] = dataset_B['Name'].apply(lambda x: unicodedata.normalize('NFKD', x).encode('ASCII', 'ignore').decode())"
      ],
      "metadata": {
        "id": "kTuCGo2sA_uJ"
      },
      "execution_count": 145,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2. Blocking:**\n",
        "\n",
        "Blocking is a technique used to reduce the number of record pairs compared for similarity. It involves grouping records into blocks based on some key attributes\n",
        "\n",
        "We'll create blocks based on a common attribute- the first letter of the last name."
      ],
      "metadata": {
        "id": "lgdOJqf-BEMG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create blocks based on the first letter of the last name\n",
        "dataset_A['Last_Name_Block'] = dataset_A['Name'].str[0]\n",
        "dataset_B['Last_Name_Block'] = dataset_B['Name'].str[0]\n",
        "# Blocking using postal codes\n",
        "blocked_pairs = []\n",
        "for address in dataset_A['Address'].unique():\n",
        "    block_A = dataset_A[dataset_A['Address'] == address]\n",
        "    block_B = dataset_B[dataset_B['Address'] == address]\n",
        "    for index_A, record_A in block_A.iterrows():\n",
        "        for index_B, record_B in block_B.iterrows():\n",
        "            blocked_pairs.append((index_A, index_B))\n"
      ],
      "metadata": {
        "id": "iKWqqcNpBInQ"
      },
      "execution_count": 157,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3. Comparison:**\n",
        "\n",
        "In this step, we compute the similarity between pairs of records based on their attributes. Common similarity metrics include:\n",
        "\n",
        "Edit distance for textual data.\n",
        "Jaccard similarity for sets.\n",
        "Cosine similarity for vectors.\n",
        "\n",
        "- We'll compare pairs of records within each block using a similarity metric like Jaccard similarity for the Name attribute.\n",
        "- We will compute similarity using edit distance for names."
      ],
      "metadata": {
        "id": "COXHGHS4BNHR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install fuzzywuzzy\n",
        "import fuzzywuzzy\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2l_CQQa7BOhf",
        "outputId": "070471ea-f51d-415d-8c3f-1911f8f3c9ed"
      },
      "execution_count": 158,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: fuzzywuzzy in /usr/local/lib/python3.10/dist-packages (0.18.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from fuzzywuzzy import fuzz\n",
        "\n",
        "# Calculate Jaccard similarity for Name attribute\n",
        "def calculate_similarity(name1, name2):\n",
        "    return fuzz.token_set_ratio(name1, name2) / 100.0\n",
        "\n",
        "# Calculate similarity scores for pairs of records\n",
        "similarity_scores = []\n",
        "for index_A, record_A in dataset_A.iterrows():\n",
        "    for index_B, record_B in dataset_B.iterrows():\n",
        "        if record_A['Last_Name_Block'] == record_B['Last_Name_Block']:\n",
        "            similarity_score = calculate_similarity(record_A['Name'], record_B['Name'])\n",
        "            similarity_scores.append({\n",
        "                'ID_A': record_A['ID'],\n",
        "                'ID_B': record_B['ID'],\n",
        "                'Similarity_Score': similarity_score\n",
        "            })\n",
        "\n",
        "# Convert the similarity scores to a DataFrame\n",
        "similarity_df = pd.DataFrame(similarity_scores)\n"
      ],
      "metadata": {
        "id": "WZ3Rrw_TBdiw"
      },
      "execution_count": 159,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**4. Classification:**\n",
        "\n",
        "We compare pairs of records based on their computed similarity scores and apply a threshold to determine potential matches.\n",
        "\n",
        "We'll classify pairs of records as matches or non-matches based on a similarity threshold."
      ],
      "metadata": {
        "id": "W9NKSN1bBh1Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set a similarity threshold\n",
        "similarity_threshold = 0.8\n",
        "\n",
        "# Classify pairs of records\n",
        "matches = similarity_df[similarity_df['Similarity_Score'] >= similarity_threshold]\n",
        "non_matches = similarity_df[similarity_df['Similarity_Score'] < similarity_threshold]\n",
        "print( matches)\n",
        "print( non_matches)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BuJq--JOBi0_",
        "outputId": "11dc52ca-d7a7-4e70-d6d8-72666cd80585"
      },
      "execution_count": 160,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   ID_A  ID_B  Similarity_Score\n",
            "0     1     5               1.0\n",
            "1     2     6               1.0\n",
            "   ID_A  ID_B  Similarity_Score\n",
            "2     4     8              0.75\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**5. Evaluation of Record Linkage:**\n",
        "\n",
        "We'll evaluate the results by comparing them to a reference dataset (ground truth) if available."
      ],
      "metadata": {
        "id": "uYxWUtKXBnY8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming we have a ground truth dataset\n",
        "ground_truth = {\n",
        "    'ID_A': [1, 2, 3],\n",
        "    'ID_B': [5, 6, 7]\n",
        "}\n",
        "\n",
        "ground_truth_df = pd.DataFrame(ground_truth)\n",
        "\n",
        "# Calculate precision, recall, and F1-score\n",
        "true_positives = len(matches.merge(ground_truth_df, on=['ID_A', 'ID_B']))\n",
        "false_positives = len(matches) - true_positives\n",
        "false_negatives = len(ground_truth_df) - true_positives\n",
        "\n",
        "precision = true_positives / (true_positives + false_positives)\n",
        "recall = true_positives / (true_positives + false_negatives)\n",
        "f1_score = 2 * (precision * recall) / (precision + recall)\n",
        "\n",
        "print(\"Precision:\", precision)\n",
        "print(\"Recall:\", recall)\n",
        "print(\"F1-score:\", f1_score)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iDJQIX0iBn9_",
        "outputId": "503e12d5-283c-4ad5-cb8f-5cc1fc4539b7"
      },
      "execution_count": 161,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Precision: 1.0\n",
            "Recall: 0.6666666666666666\n",
            "F1-score: 0.8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**6. Post-processing:**\n",
        "\n",
        "In this step, we'll resolve duplicates by merging or flagging them for further review. For demonstration purposes, let's print out the matched records and mark them as potential duplicates.\n",
        "python"
      ],
      "metadata": {
        "id": "hcoa2JQnCXKh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Print out matched records and mark them as potential duplicates\n",
        "for index, row in matches.iterrows():\n",
        "    record_A = dataset_A[dataset_A['ID'] == row['ID_A']].iloc[0]\n",
        "    record_B = dataset_B[dataset_B['ID'] == row['ID_B']].iloc[0]\n",
        "    print(\"Potential Duplicate:\")\n",
        "    print(\"Record A:\", record_A)\n",
        "    print(\"Record B:\", record_B)\n",
        "    print()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WxuV0lDcCaOZ",
        "outputId": "8ea45215-f946-4da6-de46-ece528b08819"
      },
      "execution_count": 162,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Potential Duplicate:\n",
            "Record A: ID                            1\n",
            "Name                 John Smith\n",
            "Age                          30\n",
            "Address             123 Main St\n",
            "Phone              111-111-1111\n",
            "Last_Name_Block               J\n",
            "Name: 0, dtype: object\n",
            "Record B: ID                            5\n",
            "Name                 John Smith\n",
            "Age                          30\n",
            "Address             123 Main St\n",
            "Phone              111-111-1111\n",
            "Last_Name_Block               J\n",
            "Name: 0, dtype: object\n",
            "\n",
            "Potential Duplicate:\n",
            "Record A: ID                             2\n",
            "Name               Alice Johnson\n",
            "Age                           25\n",
            "Address               456 Elm St\n",
            "Phone               222-222-2222\n",
            "Last_Name_Block                A\n",
            "Name: 1, dtype: object\n",
            "Record B: ID                             6\n",
            "Name               Alice Johnson\n",
            "Age                           26\n",
            "Address               456 Elm St\n",
            "Phone               222-222-2222\n",
            "Last_Name_Block                A\n",
            "Name: 1, dtype: object\n",
            "\n"
          ]
        }
      ]
    }
  ]
}